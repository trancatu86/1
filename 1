import numpy as np
from pathlib import Path
from typing import List, Union
from collections import namedtuple
import pandas as pd
import sys
import h5py
import os
import csv

PunchRFAHeaderBlock = namedtuple("PunchHeaderBlock",
                                 ("title", "subtitle", "label", "table_desc", "output_format", "subcase_id", "freq"))


class FileReader:
  def __init__(self, filepath: Union[Path, str]):
    self._file = self._open(filepath)
    self._filepath = filepath
  
  def _open(self, filepath: Union[Path, str]):
    raise ImplementationError
  
  def __enter__(self):
    return self
  
  def __exit__(self, exc_type, exc_val, exc_tb):
    self._file.close()


class PunchReader(FileReader):
  def _open(self, filepath: Union[Path, str]):
    return open(filepath, "r")

  def _get_header_block(self, header_title: str):
    header_block = [header_title[:-9]]
    while True:
      anchor = self._file.tell()
      eof, header_line = self._read_line()
      if eof:
        break
      if not header_line.startswith("$"):
        self._file.seek(anchor)
        break
      header_block.append(header_line[:-9])
    return header_block

  def _parse_fra_header_block(self, header_block: List[str]):
    title = "=".join(header_block[0].split("=")[1:]).strip()
    subtitle = "=".join(header_block[1].split("=")[1:]).strip()
    label = "=".join(header_block[2].split("=")[1:]).strip()
    table_desc = header_block[3].strip()
    output_format = header_block[4].strip()
    subcase_id = int(header_block[5].split("=")[-1].strip())
    freq = float(header_block[6].split("=")[-1].strip())
    return PunchRFAHeaderBlock(title, subtitle, label, table_desc, output_format, subcase_id, freq)

  def _read_line(self):
    eof = False
    rawline: str = self._file.readline()
    if not len(rawline):
      eof = True
      rawline = ""
    return (eof, rawline)

  def _skip_to_next_header_block(self):
    while True:
      anchor = self._file.tell()
      eof, rawline = self._read_line()
      if eof: break 
      if not rawline.startswith("$"):
        continue
      self._file.seek(anchor)
      break
    return

  def parse(self):
    """
    data = {subcase_id: {
                enforced_nid: int,
                enforced_label: str,
                node_id: {
                    result_type['pres'|'acc']: np.ndarray
                    }
                }
            }
    """
    header = None
    data = {}
    cur_freq = None
    cur_excitation = None
    cur_node_id = None
    cur_subcase_id = None
    cur_result_type = None
    
    while True:
      eof, rawline = self._read_line()
      if eof: break
      if rawline.startswith("$"):
        header_block = self._get_header_block(rawline)
        if len(header_block) != 7:  # uninteresting data block
          self._skip_to_next_header_block()
          continue

        header = self._parse_fra_header_block(header_block)
        if header.output_format.upper() != "$REAL-IMAGINARY OUTPUT":  # uninteresting data block
          self._skip_to_next_header_block()
          continue
        
        cur_freq = header.freq
        enforced_label = header.label[-2:].lower()
        cur_subcase_id = header.subcase_id
        enforced_nid = int(header.label.split("_")[-1][1:-3])

        if cur_subcase_id not in data:
          data[cur_subcase_id] = {"enforced_nid": enforced_nid, "enforced_label": enforced_label}
        cur_result_type = "pres" if header.table_desc.upper() == "$DISPLACEMENTS" else "acc"
        continue

      line = rawline[:-9]
      if line.startswith(" "):
        if cur_result_type == "pres":
          line = line.strip().replace("S", " ")
        else:
          line = line.strip().replace("G", " ")
        linedata = line.split()
        cur_node_id = int(linedata[0])

        subcase_data = data[cur_subcase_id]
        if cur_node_id not in subcase_data:
          subcase_data[cur_node_id] = {}
        node_data = subcase_data[cur_node_id]

        if cur_result_type not in subcase_data[cur_node_id]:
          node_data[cur_result_type] = []

        insert_data = [cur_freq] + list(map(float, linedata[1:]))
        node_data[cur_result_type].append(insert_data)
        continue

      if line.startswith("-"):
        line = line[6:].strip()
        insert_data = list(map(float, line.split()))
        node_data = data[cur_subcase_id][cur_node_id]
        node_data[cur_result_type][-1].extend(insert_data)
        continue

    # combine data
    for subcase_data in data.values():
      enforced_nid = subcase_data["enforced_nid"]
      for node_id, node_response in subcase_data.items():
        if node_id == "enforced_nid" or node_id == "enforced_label":
          continue
        for result_type in node_response.keys():
          response_array = np.array(node_response[result_type])
          if not response_array.size:
            node_response[result_type] = response_array
          else:
            response_array = response_array[:, [0,
                                                1, 7, 2, 8,
                                                3, 9, 4, 10,
                                                5, 11, 6, 12]]
            complex_response = response_array[:, 1::2] + 1.j * response_array[:, 2::2]
            mag = np.abs(complex_response)
            phase = np.angle(complex_response, True)
            response_array = np.concatenate([response_array, mag, phase], axis=-1)
            response_array = response_array[:, [0,
                                                1, 2, 13, 19,
                                                3, 4, 14, 20,
                                                5, 6, 15, 21,
                                                7, 8, 16, 22,
                                                9, 10, 17, 23,
                                                11, 12, 18, 24]]
            if result_type == "pres":
              node_response[result_type] = response_array[:, :5]
            else:
              node_response[result_type] = response_array

    for subcase_id, subcase_data in data.items():
      data_collated = []
      column_names = ["freq"]
      enforced_nid = subcase_data["enforced_nid"]
      enforced_label = subcase_data["enforced_label"]
      for idx, (node_id, node_response) in enumerate(subcase_data.items()):
        if node_id == "enforced_nid" or node_id == "enforced_label":
          continue
        for result_type, response_data in node_response.items():
          if response_data.size:
            data_collated.append(response_data[:, int(len(data_collated) > 0):])
            if result_type == "pres":
              column_names.extend([u + c for u in [f"{node_id}.px"]
                                   for c in [".R", ".I", ".Mag", ".Phase"]
                                   ])
            else:
              column_names.extend([u + c
                                   for u in [f"{node_id}.ax", f"{node_id}.ay", f"{node_id}.az", f"{node_id}.rx", f"{node_id}.ry", f"{node_id}.rz"]
                                   for c in [".R", ".I", ".Mag", ".Phase"]
                                   ])

      data_collated = np.concatenate(data_collated, axis=-1)
      data[subcase_id] = {"enforced_nid": enforced_nid,
                          "enforced_label": enforced_label,
                          "response": pd.DataFrame(data_collated, columns=column_names)}
    return data


def cast_to_structured_array(df: pd.DataFrame):    
  dtype = np.dtype([(colname, df[colname].dtype) for colname in df.columns])    
  return np.core.records.fromarrays(df.to_numpy().transpose(), dtype=dtype)
current_path = os.getcwd()

def build_database_punch(save_to, num_mounts, model_name, eng_pch, body_pch, avl_files,
                   mount_params, node_ids, response_ids, engine_angles, remarks,
                   mount_param_type='kelvin-voigt'):
  dname = "/NODE_IDS"
  with h5py.File(save_to, "w") as df:
    df.attrs["num_mounts"] = num_mounts
    df.attrs["mount_param_type"] = mount_param_type
    df.attrs["model_name"] = model_name
    df.attrs["eng_pch"] = os.path.basename(eng_pch)
    df.attrs["body_pch"] = os.path.basename(body_pch)
    df.attrs["remarks"] = remarks
    df.attrs["engine_angles"] = engine_angles
    dtype = np.dtype( [("eng_nids", "i8"), ("body_nids", "i8"), ("avl_nids", "i8")] )
    dset = df.create_dataset(dname, data=np.core.records.fromarrays(node_ids, dtype=dtype))

    engine_response_nids = set()
    with PunchReader(eng_pch) as f:
      eng_data = f.parse()
      eng_nids = node_ids[0, :].tolist()
      for subcase_id, subcase_data in eng_data.items():
        enforced_nid = subcase_data["enforced_nid"]
        enforced_label = subcase_data["enforced_label"]
        if enforced_nid in eng_nids:
          dname = f"/EngMount/N{enforced_nid}/{enforced_label.upper()}"
          dset = df.create_dataset(dname, data=cast_to_structured_array(subcase_data["response"]))
          response_nids = [int(name.split(".")[0]) for name in subcase_data['response'].columns if not name.startswith("freq")]
          engine_response_nids |= set(response_nids)

    body_response_nids = set()
    with PunchReader(body_pch) as f:
      body_data = f.parse()
      body_nids = node_ids[1, :].tolist()
      for subcase_id, subcase_data in body_data.items():
        enforced_nid = subcase_data["enforced_nid"]
        enforced_label = subcase_data["enforced_label"]
        if enforced_nid in body_nids:
          dname = f"/BodyMount/N{enforced_nid}/{enforced_label.upper()}"
          dset = df.create_dataset(dname, data=cast_to_structured_array(subcase_data["response"]))
          for name in subcase_data["response"].columns:
            if name == "freq":
              continue
            nid, lbl, *_ = name.split(".")
            if lbl != "px":
              body_response_nids.add(int(nid))

    response_ids_set = set(response_ids)
    df.attrs['response_id'] = list(response_ids_set.difference(engine_response_nids, body_response_nids))  # the name `response_id` is for backward-compatible
    df.attrs['response_id_eng'] = list(response_ids_set & engine_response_nids)
    df.attrs['response_id_body'] = list(response_ids_set & body_response_nids)

    nast_freq = next(iter(eng_data.values()))["response"]["freq"].to_numpy()
    avldata = {}
    column_names = {}
    axis_labels = ("ax", "ay", "az", "rx", "ry", "rz")
    avl_nids = node_ids[2, :].tolist()
    for fpath in avl_files:
      fname = os.path.splitext(os.path.basename(fpath))[0]
      name_components = fname.split("_")
      axis = axis_labels[int(name_components[-2][-1]) - 1]
      rpm = int(name_components[-1].strip("rpm"))
      mount_nid = int(name_components[-4])
      if mount_nid not in avl_nids:
        continue

      if rpm not in avldata:
        avldata[rpm] = []
        column_names[rpm] = []

      data = pd.read_csv(fpath, skiprows=[1]).to_numpy()
      avl_freq = data[:, 0]
      mag = data[:, 1]
      phase = np.radians(data[:, 2])
      # interp_data = np.interp(nast_freq, avl_freq, mag * np.cos(phase) + 1.j * mag * np.sin(phase) )
      avl_resp = mag * np.cos(phase) + 1.j * mag * np.sin(phase)
      new_colnames = [f"{mount_nid}.{axis}.{comp}" for comp in ("R", "I", "Mag", "Phase")]

      if len(avldata[rpm]):
        # avldata[rpm].extend([interp_data.real, interp_data.imag, np.abs(interp_data), np.angle(interp_data, True)])
        avldata[rpm].extend([avl_resp.real, avl_resp.imag, np.abs(avl_resp), np.angle(avl_resp, True)])
        column_names[rpm].extend(new_colnames)
      else:
        # avldata[rpm].append([nast_freq, interp_data.real, interp_data.imag, np.abs(interp_data), np.angle(interp_data, True)])
        avldata[rpm].append([avl_freq, avl_resp.real, avl_resp.imag, np.abs(avl_resp), np.angle(avl_resp, True)])
        column_names[rpm].extend(["freq"] + new_colnames)

    for rpm, data in avldata.items():
      dname = f"/AVLMount/{rpm}RPM"
      collated_data = pd.DataFrame(np.vstack(data).T, columns=column_names[rpm])
      dset = df.create_dataset(dname, data=cast_to_structured_array(collated_data))

    if mount_param_type == 'kelvin-voigt':
      dname = f"/MOUNT_PARAMS"
      column_names = []
      params = []
      for mount_nid, mount_elems in mount_params.items():
        for elname, eldata in mount_elems.items():
          column_names.extend([f"{mount_nid}.{elname}.{elcomp}" for elcomp in eldata.keys()])
          params.extend( list(eldata.values()) )
      dtype = np.dtype([(colname, "f8") for colname in column_names])
      params = np.array([tuple(params)], dtype=dtype)
      dset = df.create_dataset( dname, data=params )
    else:
      # tabular dynamic stiffness
      column_names = ["freq", "real", "imag"]
      for mount_nid, mount_input_data in mount_params.items():
        group_name = f"/MountParams/n{mount_nid}"
        group  = df.create_group(group_name)
        if mount_input_data["mountType"] == "Rigid":
          group.attrs["mount_type"] = "rigid"
          group.attrs["matrix_type"] = "none"
          continue

        group.attrs["mount_type"] = "flexible"
        group.attrs["matrix_type"] = mount_input_data["matrixType"]
        group.attrs["fixed_in_excite"] = mount_input_data["fixedInExcite"]
        stiffness_data = mount_input_data["dynStiffness"]
        for axis_label in stiffness_data:
          dname = f"{group_name}/{axis_label.upper()}"
          dyn_stiff_table = np.array(stiffness_data[axis_label])
          mount_data = pd.DataFrame(data=dyn_stiff_table, columns=column_names)
          dset = df.create_dataset( dname, data=cast_to_structured_array(mount_data) )
  return

def build_database_csv(save_to, num_mounts, model_name, eng_pch, body_csv, avl_files,
                   mount_params, node_ids, response_ids, engine_angles, remarks,
                   mount_param_type='kelvin-voigt'):
  dname = "/NODE_IDS"
  with h5py.File(save_to, "w") as df:
    df.attrs["num_mounts"] = num_mounts
    df.attrs["mount_param_type"] = mount_param_type
    df.attrs["model_name"] = model_name
    df.attrs["eng_pch"] = os.path.basename(eng_pch)
    df.attrs["body_csv"] = ""
    df.attrs["remarks"] = remarks
    df.attrs["engine_angles"] = engine_angles
    dtype = np.dtype( [("eng_nids", "i8"), ("body_nids", "i8"), ("avl_nids", "i8")] )
    dset = df.create_dataset(dname, data=np.core.records.fromarrays(node_ids, dtype=dtype))

    engine_response_nids = set()
    with PunchReader(eng_pch) as f:
      eng_data = f.parse()
      eng_nids = node_ids[0, :].tolist()
      for subcase_id, subcase_data in eng_data.items():
        enforced_nid = subcase_data["enforced_nid"]
        enforced_label = subcase_data["enforced_label"]
        if enforced_nid in eng_nids:
          dname = f"/EngMount/N{enforced_nid}/{enforced_label.upper()}"
          dset = df.create_dataset(dname, data=cast_to_structured_array(subcase_data["response"]))
          response_nids = [int(name.split(".")[0]) for name in subcase_data['response'].columns if not name.startswith("freq")]
          engine_response_nids |= set(response_nids)

#############

    body_response_nids = set()
    body_microphone_response_nids = set()
    microphone_response_nids = set()    
    nast_freq = next(iter(eng_data.values()))["response"]["freq"].to_numpy()
    bodydata = {}
    column_names = {}   
    with open(body_csv[0], 'r') as f:
      column_line = f.readline().strip()
    column = column_line.split(',')
#    print("len(column)")    
#    print(len(column))
    loop_number=(len(column)-1)/4
#    print("loop_number")    
#    print(loop_number)    
    for name in column:          
      if name == "freq":
        continue
      nid, lbl, MagPhase = name.split(".")
      if lbl == "ax" and MagPhase=="Mag":
        body_response_nids.add(int(nid))
      if lbl == "px" and MagPhase=="Mag":
        microphone_response_nids.add(int(nid)) 
          
#    print("body_response_nids_database_builder")
#    print(body_response_nids) 
#    print("microphone_response_nids_database_builder")
#    print(microphone_response_nids)   
    rpm=0    
  
    for fpath in body_csv:
      fname = os.path.splitext(os.path.basename(fpath))[0]
      name_components = fname.split("_")
      body_nids = node_ids[1, :].tolist()  # ボディノードIDリスト
      enforced_nid = name_components[1][1:]
      enforced_label = name_components[-1]
      data = pd.read_csv(fpath).to_numpy()

      bodydata[rpm] = []
      column_names[rpm] = []  
      for k in range(0,int(loop_number)):
        body_freq = data[:, 0]
        body_resp_1 = data[:, 4*k+1]
        body_resp_2 = data[:, 4*k+2]
        body_resp_3 = data[:, 4*k+3]
        body_resp_4 = data[:, 4*k+4] 
#        print("body_resp_1 k")  
#        print(fname)
#        print(k)    
#        print(data[2, 4*k+1])
        new_colnames = [column[4*k+1],column[4*k+2],column[4*k+3],column[4*k+4]]
        if len(bodydata[rpm]):
          bodydata[rpm].extend([body_resp_1, body_resp_2,body_resp_3, body_resp_4])
          column_names[rpm].extend(new_colnames)
        else:
          bodydata[rpm].append([body_freq,body_resp_1,body_resp_2,body_resp_3, body_resp_4])
          column_names[rpm].extend(["freq"] + new_colnames)
          
      for rpm, data in bodydata.items():
        dname = f"/BodyMount/N{enforced_nid}/{enforced_label.upper()}"
        collated_data = pd.DataFrame(np.vstack(data).T, columns=column_names[rpm])      
        dset = df.create_dataset(dname, data=cast_to_structured_array(collated_data))        
        
    response_ids_set = set(response_ids)
    df.attrs['response_id'] = list(response_ids_set.difference(engine_response_nids, body_response_nids))  # エンジン・ボディ以外
    df.attrs['response_id_eng'] = list(response_ids_set & engine_response_nids)  # エンジン関連
    df.attrs['response_id_body'] = list(response_ids_set & body_response_nids)  # ボディ関連


###########

    nast_freq = next(iter(eng_data.values()))["response"]["freq"].to_numpy()
    avldata = {}
    column_names = {}
    axis_labels = ("ax", "ay", "az", "rx", "ry", "rz")
    avl_nids = node_ids[2, :].tolist()
    for fpath in avl_files:
      fname = os.path.splitext(os.path.basename(fpath))[0]
      name_components = fname.split("_")
      axis = axis_labels[int(name_components[-2][-1]) - 1]
      rpm = int(name_components[-1].strip("rpm"))
      mount_nid = int(name_components[-4])
      if mount_nid not in avl_nids:
        continue

      if rpm not in avldata:
        avldata[rpm] = []
        column_names[rpm] = []

      data = pd.read_csv(fpath, skiprows=[1]).to_numpy()
      avl_freq = data[:, 0]
      mag = data[:, 1]
      phase = np.radians(data[:, 2])
      # interp_data = np.interp(nast_freq, avl_freq, mag * np.cos(phase) + 1.j * mag * np.sin(phase) )
      avl_resp = mag * np.cos(phase) + 1.j * mag * np.sin(phase)
      new_colnames = [f"{mount_nid}.{axis}.{comp}" for comp in ("R", "I", "Mag", "Phase")]

      if len(avldata[rpm]):
        # avldata[rpm].extend([interp_data.real, interp_data.imag, np.abs(interp_data), np.angle(interp_data, True)])
        avldata[rpm].extend([avl_resp.real, avl_resp.imag, np.abs(avl_resp), np.angle(avl_resp, True)])
        column_names[rpm].extend(new_colnames)
      else:
        # avldata[rpm].append([nast_freq, interp_data.real, interp_data.imag, np.abs(interp_data), np.angle(interp_data, True)])
        avldata[rpm].append([avl_freq, avl_resp.real, avl_resp.imag, np.abs(avl_resp), np.angle(avl_resp, True)])
        column_names[rpm].extend(["freq"] + new_colnames)

    for rpm, data in avldata.items():
      dname = f"/AVLMount/{rpm}RPM"
      collated_data = pd.DataFrame(np.vstack(data).T, columns=column_names[rpm])
      dset = df.create_dataset(dname, data=cast_to_structured_array(collated_data))

    if mount_param_type == 'kelvin-voigt':
      dname = f"/MOUNT_PARAMS"
      column_names = []
      params = []
      for mount_nid, mount_elems in mount_params.items():
        for elname, eldata in mount_elems.items():
          column_names.extend([f"{mount_nid}.{elname}.{elcomp}" for elcomp in eldata.keys()])
          params.extend( list(eldata.values()) )
      dtype = np.dtype([(colname, "f8") for colname in column_names])
      params = np.array([tuple(params)], dtype=dtype)
      dset = df.create_dataset( dname, data=params )
    else:
      # tabular dynamic stiffness
      column_names = ["freq", "real", "imag"]
      for mount_nid, mount_input_data in mount_params.items():
        group_name = f"/MountParams/n{mount_nid}"
        group  = df.create_group(group_name)
        if mount_input_data["mountType"] == "Rigid":
          group.attrs["mount_type"] = "rigid"
          group.attrs["matrix_type"] = "none"
          continue

        group.attrs["mount_type"] = "flexible"
        group.attrs["matrix_type"] = mount_input_data["matrixType"]
        group.attrs["fixed_in_excite"] = mount_input_data["fixedInExcite"]
        stiffness_data = mount_input_data["dynStiffness"]
        for axis_label in stiffness_data:
          dname = f"{group_name}/{axis_label.upper()}"
          dyn_stiff_table = np.array(stiffness_data[axis_label])
          mount_data = pd.DataFrame(data=dyn_stiff_table, columns=column_names)
          dset = df.create_dataset( dname, data=cast_to_structured_array(mount_data) )
  return
  
#############################################################
def build_database_punch_BlockedForce_acoustic(save_to, body_pch, node_ids, response_ids):
  print("build_database_punch_BlockedForce_acoustic") 
  dname = "/NODE_IDS"
  h5_path= current_path + "\\db\\database.h5"  
  with h5py.File(save_to, "w") as df:
    dtype = np.dtype( [("body_nids", "i8")] )
    dset = df.create_dataset(dname, data=np.core.records.fromarrays(node_ids, dtype=dtype))

    body_response_nids = set()
    with PunchReader(body_pch) as f:
      body_data = f.parse()
      body_nids = node_ids[0, :].tolist()
      for subcase_id, subcase_data in body_data.items():
        enforced_nid = subcase_data["enforced_nid"]
#        print("enforced_nid") 
#        print(enforced_nid)        
        enforced_label = subcase_data["enforced_label"]
#        print("enforced_label")        
#        print(enforced_label)       
        
        if enforced_nid in body_nids:
          dname = f"/BodyMount/N{enforced_nid}/{enforced_label.upper()}"     
          
          dset = df.create_dataset(dname, data=cast_to_structured_array(subcase_data["response"]))
          for name in subcase_data["response"].columns:
            if name == "freq":
              continue
            nid, lbl, *_ = name.split(".")
            if lbl != "px":
              body_response_nids.add(int(nid))
  print("response_ids")
  print(response_ids) 
  response_ids_str = [str(i) for i in response_ids]  
  

  output_csv = current_path + "\\BlockedForce\\bf_body_blocked_force_punch"
      
  export_all_datasets(h5_path, output_csv)  

  convert_complex_acoustic_only(response_ids_str,output_csv)
  os.remove(h5_path)  
  return  
  

#############################################################
def build_database_punch_BlockedForce(save_to, body_pch, node_ids, response_ids, body_responseId_nids):
  print("build_database_punch_BlockedForce")     
  dname = "/NODE_IDS"
  with h5py.File(save_to, "w") as df:
    dtype = np.dtype( [("body_nids", "i8")] )
    dset = df.create_dataset(dname, data=np.core.records.fromarrays(node_ids, dtype=dtype))

    body_response_nids = set()
    with PunchReader(body_pch) as f:
      body_data = f.parse()
      body_nids = node_ids[0, :].tolist()
      for subcase_id, subcase_data in body_data.items():
        enforced_nid = subcase_data["enforced_nid"]
#        print("enforced_nid") 
#        print(enforced_nid)        
        enforced_label = subcase_data["enforced_label"]
#        print("enforced_label")        
#        print(enforced_label)       
        
        if enforced_nid in body_nids:
          dname = f"/BodyMount/N{enforced_nid}/{enforced_label.upper()}"     
          
          dset = df.create_dataset(dname, data=cast_to_structured_array(subcase_data["response"]))
          for name in subcase_data["response"].columns:
            if name == "freq":
              continue
            nid, lbl, *_ = name.split(".")
            if lbl != "px":
              body_response_nids.add(int(nid))
  print("response_ids")
  print(response_ids) 
  response_ids_str = [str(i) for i in response_ids] 
  if body_responseId_nids!="":  
      body_response_ids_str = [str(i) for i in body_responseId_nids]   
    #  response_ids_str.extend(body_response_ids_str)
      h5_path= current_path + "\\db\\database.h5"
      output_csv = current_path + "\\BlockedForce\\bf_body_blocked_force_punch"
      export_all_datasets(h5_path, output_csv)  

      convert_complex(response_ids_str,output_csv)
      convert_complex_structure_ax(body_responseId_nids,output_csv) 
      convert_complex(response_ids_str,output_csv)
      convert_complex_structure_ay(body_responseId_nids,output_csv) 
      convert_complex(response_ids_str,output_csv)
      convert_complex_structure_az(body_responseId_nids,output_csv) 
  
      os.remove(h5_path)  
  return  
#############################################################
def build_database_punch_BlockedForce_body(save_to, body_pch, node_ids, body_responseId_nids):
  print("build_database_punch_BlockedForce_body")    
  dname = "/NODE_IDS"
  with h5py.File(save_to, "w") as df:
    dtype = np.dtype( [("body_nids", "i8")] )
    dset = df.create_dataset(dname, data=np.core.records.fromarrays(node_ids, dtype=dtype))
    body_response_nids = set()
    with PunchReader(body_pch) as f:
      body_data = f.parse()
      body_nids = node_ids[0, :].tolist()
      for subcase_id, subcase_data in body_data.items():
        enforced_nid = subcase_data["enforced_nid"]
#        print("enforced_nid") 
#        print(enforced_nid)        
        enforced_label = subcase_data["enforced_label"]
#        print("enforced_label")        
#        print(enforced_label)       
        
        if enforced_nid in body_nids:
          dname = f"/BodyMount/N{enforced_nid}/{enforced_label.upper()}"     
          dset = df.create_dataset(dname, data=cast_to_structured_array(subcase_data["response"]))
          for name in subcase_data["response"].columns:
            if name == "freq":
              continue
            nid, lbl, *_ = name.split(".")
            if lbl != "px":
              body_response_nids.add(int(nid))

  if body_responseId_nids!="":  
      body_response_ids_str = [str(i) for i in body_responseId_nids]   
    #  response_ids_str.extend(body_response_ids_str)
      h5_path= current_path + "\\db\\database.h5"
      output_csv = current_path + "\\BlockedForce\\bf_body_blocked_force_punch"
      export_all_datasets(h5_path, output_csv)  
      convert_complex_structure_ax(body_responseId_nids,output_csv) 
      convert_complex_structure_ay(body_responseId_nids,output_csv) 
      convert_complex_structure_az(body_responseId_nids,output_csv) 
  
      os.remove(h5_path)  
  return
  
def combine_complex_columns(df, prefix):
    real_col = f"{prefix}.px.R"
    imag_col = f"{prefix}.px.I"
    complex_col_name = prefix  # Tên cột số phức mới

    if real_col in df.columns and imag_col in df.columns:
        df[complex_col_name] = df[real_col] + 1j * df[imag_col]
        df.drop(columns=[real_col, imag_col], inplace=True)
    return df

def convert_complex(response_ids, input_folder):
    for filename in os.listdir(input_folder):
        if filename.endswith('.csv'):
            file_path = os.path.join(input_folder, filename)
            df = pd.read_csv(file_path)

            # Lọc cột giữ lại: cột 'freq' và các cột bắt đầu bằng phần tử trong response_ids
            cols_to_keep = ['freq']
            for prefix in response_ids:
                cols_to_keep += [col for col in df.columns if col.startswith(prefix + ".")]

            df_filtered = df[cols_to_keep].copy()

            # Ghép các cột R và I thành số phức
            for prefix in response_ids:
                df_filtered = combine_complex_columns(df_filtered, prefix)

            # Chỉ giữ lại cột freq và các cột số phức mới (tên là prefix)
            cols_final = ['freq'] + response_ids
            df_final = df_filtered[cols_final]

            # Lưu file kết quả
            output_folder = current_path +"\\BlockedForce\\body_blocked_force_convert_complex_acoustic"
            os.makedirs(output_folder, exist_ok=True)
            output_path = os.path.join(output_folder, filename)
            df_final.to_csv(output_path, index=False)
            
def convert_complex_acoustic_only(response_ids, input_folder):
    for filename in os.listdir(input_folder):
        if filename.endswith('.csv'):
            file_path = os.path.join(input_folder, filename)
            df = pd.read_csv(file_path)

            # Lọc cột giữ lại: cột 'freq' và các cột bắt đầu bằng phần tử trong response_ids
            cols_to_keep = ['freq']
            for prefix in response_ids:
                cols_to_keep += [col for col in df.columns if col.startswith(prefix + ".")]

            df_filtered = df[cols_to_keep].copy()

            # Ghép các cột R và I thành số phức
            for prefix in response_ids:
                df_filtered = combine_complex_columns(df_filtered, prefix)

            # Chỉ giữ lại cột freq và các cột số phức mới (tên là prefix)
            cols_final = ['freq'] + response_ids
            df_final = df_filtered[cols_final]

            # Lưu file kết quả
            output_folder = current_path +"\\BlockedForce\\body_blocked_force_convert_complex"
            os.makedirs(output_folder, exist_ok=True)
            output_path = os.path.join(output_folder, filename)
            df_final.to_csv(output_path, index=False)
            
def combine_complex_columns_structure_ax(df, prefix):
    real_col = f"{prefix}.ax.R"
    imag_col = f"{prefix}.ax.I"
    complex_col_name = prefix  # Tên cột số phức mới
    if real_col in df.columns and imag_col in df.columns:
        df[complex_col_name] = df[real_col] + 1j * df[imag_col]
        df.drop(columns=[real_col, imag_col], inplace=True)
    return df
def combine_complex_columns_structure_ay(df, prefix):
    real_col = f"{prefix}.ay.R"
    imag_col = f"{prefix}.ay.I"
    complex_col_name = prefix  # Tên cột số phức mới
    if real_col in df.columns and imag_col in df.columns:
        df[complex_col_name] = df[real_col] + 1j * df[imag_col]
        df.drop(columns=[real_col, imag_col], inplace=True)
    return df
def combine_complex_columns_structure_az(df, prefix):
    real_col = f"{prefix}.az.R"
    imag_col = f"{prefix}.az.I"
    complex_col_name = prefix  # Tên cột số phức mới
    if real_col in df.columns and imag_col in df.columns:
        df[complex_col_name] = df[real_col] + 1j * df[imag_col]
        df.drop(columns=[real_col, imag_col], inplace=True)
    return df
    
    
def convert_complex_structure_ax(response_ids, input_folder):
    output_folder = current_path +"\\BlockedForce\\body_blocked_force_convert_complex_structure_ax"     
    for filename in os.listdir(input_folder):
        if filename.endswith('.csv'):
            file_path = os.path.join(input_folder, filename)
            df = pd.read_csv(file_path)
            # Lọc cột giữ lại: cột 'freq' và các cột bắt đầu bằng phần tử trong response_ids
            cols_to_keep = ['freq']
            for prefix in response_ids:
                cols_to_keep += [str(col) for col in df.columns if str(col).startswith(str(prefix) + ".")]
   
            df_filtered = df[cols_to_keep].copy()
            # Ghép các cột R và I thành số phức
            for prefix in response_ids:
                df_filtered = combine_complex_columns_structure_ax(df_filtered, prefix)

            # Chỉ giữ lại cột freq và các cột số phức mới (tên là prefix)
            cols_final = ['freq'] + response_ids
            df_final = df_filtered[cols_final]


            os.makedirs(output_folder, exist_ok=True)
            output_path = os.path.join(output_folder, filename)
            df_final.to_csv(output_path, index=False)      
    rename_columns_based_on_folder(output_folder)  
            
def convert_complex_structure_ay(response_ids, input_folder):
    output_folder = current_path +"\\BlockedForce\\body_blocked_force_convert_complex_structure_ay"             
    for filename in os.listdir(input_folder):
        if filename.endswith('.csv'):
            file_path = os.path.join(input_folder, filename)
            df = pd.read_csv(file_path)
            # Lọc cột giữ lại: cột 'freq' và các cột bắt đầu bằng phần tử trong response_ids
            cols_to_keep = ['freq']
            for prefix in response_ids:
                cols_to_keep += [str(col) for col in df.columns if str(col).startswith(str(prefix) + ".")]
   
            df_filtered = df[cols_to_keep].copy()
            # Ghép các cột R và I thành số phức
            for prefix in response_ids:
                df_filtered = combine_complex_columns_structure_ay(df_filtered, prefix)

            # Chỉ giữ lại cột freq và các cột số phức mới (tên là prefix)
            cols_final = ['freq'] + response_ids
            df_final = df_filtered[cols_final]

            os.makedirs(output_folder, exist_ok=True)
            output_path = os.path.join(output_folder, filename)
            df_final.to_csv(output_path, index=False)          
    rename_columns_based_on_folder(output_folder)        
            
def convert_complex_structure_az(response_ids, input_folder):
    output_folder = current_path +"\\BlockedForce\\body_blocked_force_convert_complex_structure_az"    
    for filename in os.listdir(input_folder):
        if filename.endswith('.csv'):
            file_path = os.path.join(input_folder, filename)
            df = pd.read_csv(file_path)

            cols_to_keep = ['freq']
            for prefix in response_ids:
                cols_to_keep += [str(col) for col in df.columns if str(col).startswith(str(prefix) + ".")]
   
            df_filtered = df[cols_to_keep].copy()

            for prefix in response_ids:
                df_filtered = combine_complex_columns_structure_az(df_filtered, prefix)

            cols_final = ['freq'] + response_ids
            df_final = df_filtered[cols_final]


            os.makedirs(output_folder, exist_ok=True)
            output_path = os.path.join(output_folder, filename)
            df_final.to_csv(output_path, index=False) 
    rename_columns_based_on_folder(output_folder)


def rename_columns_based_on_folder(folder_path):
    # Lấy 2 ký tự cuối cùng của tên folder
    folder_suffix = os.path.basename(folder_path)[-2:].lower()

    for filename in os.listdir(folder_path):
        if filename.endswith('.csv'):
            file_path = os.path.join(folder_path, filename)
            df = pd.read_csv(file_path)

            cols = df.columns.tolist()

            # Đổi tên các cột từ cột thứ 2 trở đi
            new_cols = [cols[0]] + [col + f"_{folder_suffix}" for col in cols[1:]]

            df.columns = new_cols

            df.to_csv(file_path, index=False)


            
def merge_csv_files(folders, output_folder):
    """
    folders: list các đường dẫn folder cần merge, ví dụ [folder1, folder2, folder3, folder4]
    output_folder: folder lưu kết quả
    """
    os.makedirs(output_folder, exist_ok=True)

    # Lấy danh sách file trong từng folder, lưu dưới dạng set
    list_of_file_sets = [set(os.listdir(folder)) for folder in folders]

    # Tìm các file trùng tên trong tất cả folder (giao của tất cả set)
    common_files = set.intersection(*list_of_file_sets)

    for filename in common_files:
        dfs = []
        freq_col = None

        for folder in folders:
            file_path = os.path.join(folder, filename)
            df = pd.read_csv(file_path)

            # Lấy cột freq từ file đầu tiên
            if freq_col is None:
                freq_col = df[['freq']]

            # Lấy các cột khác (ngoại trừ freq)
            df_data = df.drop(columns=['freq'])
            dfs.append(df_data)

        # Ghép cột freq và các dữ liệu từ tất cả các file
        df_merged = pd.concat([freq_col] + dfs, axis=1)

        # Lưu file kết quả
        output_path = os.path.join(output_folder, filename)
        df_merged.to_csv(output_path, index=False)

           
   
def sanitize_filename(name):
    # Loại bỏ dấu '/' đầu và thay bằng '_', để làm tên file hợp lệ
    parts = name.split('/')
    if len(parts) == 3:
        temp=  parts[1] + "_"+  parts[2]
        final_name= temp.replace('N', '')
    else:
        final_name = name.strip('/').replace('/', '_')
    # Kiểm tra đủ 3 phần
    return final_name

def export_dataset_to_csv(dataset, filename):
    data = dataset[()]
    # Kiểm tra kiểu dữ liệu
    if data.dtype.names is not None:
        # Dataset structured (compound) có nhiều trường
        field_names = data.dtype.names
    
        # Nếu data là mảng 1 chiều hoặc nhiều chiều
        # Chuyển data thành mảng 2 chiều: mỗi hàng là một record, mỗi cột là một field
        if data.ndim == 0:
            # Scalar structured, chuyển thành 1 record
            rows = [[data[field] for field in field_names]]
        else:
            rows = []
            for record in data:
                rows.append([record[field] for field in field_names])
    else:
        # Dataset không có tên trường, coi như 1 biến 'value'
        field_names = ['value']
        if np.isscalar(data):
            rows = [[data]]
        elif data.ndim == 1:
            rows = [[v] for v in data]
        else:
            # Nếu nhiều chiều, flatten thành 1 chiều
            flat = data.flatten()
            rows = [[v] for v in flat]

    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(field_names)  # header
        writer.writerows(rows)

def export_all_datasets(h5file_path, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with h5py.File(h5file_path, 'r') as f:
        def visitor(name, obj):
            if isinstance(obj, h5py.Dataset):
                filename = sanitize_filename(name) + '.csv'
                if filename != "NODE_IDS.csv":
                    filepath = os.path.join(output_dir, filename)
                    export_dataset_to_csv(obj, filepath)
        f.visititems(visitor)
  
def build_database_csv_BlockedForce(save_to, body_csv, avl_files,node_ids, response_ids):
  dname = "/NODE_IDS"
  with h5py.File(save_to, "w") as df:
#    df.attrs["num_mounts"] = num_mounts
    df.attrs["body_csv"] = ""
    dtype = np.dtype( [("body_nids", "i8")] )
    dset = df.create_dataset(dname, data=np.core.records.fromarrays(node_ids, dtype=dtype))

#############

    body_response_nids = set()
    body_microphone_response_nids = set()
    microphone_response_nids = set()    
    bodydata = {}
    column_names = {}   
    with open(body_csv[0], 'r') as f:
      column_line = f.readline().strip()
    column = column_line.split(',')
#    print("len(column)")    
#    print(len(column))
    loop_number=(len(column)-1)/4
#    print("loop_number")    
#    print(loop_number)    
    for name in column:          
      if name == "freq":
        continue
      nid, lbl, MagPhase = name.split(".")
      if lbl == "ax" and MagPhase=="Mag":
        body_response_nids.add(int(nid))
      if lbl == "px" and MagPhase=="Mag":
        microphone_response_nids.add(int(nid)) 
          
#    print("body_response_nids_database_builder")
#    print(body_response_nids) 
#    print("microphone_response_nids_database_builder")
#    print(microphone_response_nids)   
    rpm=0    
  
    for fpath in body_csv:
      fname = os.path.splitext(os.path.basename(fpath))[0]
      name_components = fname.split("_")
      body_nids = node_ids[0, :].tolist()  # ボディノードIDリスト
#      print("body_nids")
#      print(body_nids)
      enforced_nid = name_components[1][1:]
#      print("enforced_nid")
#      print(enforced_nid)      
      enforced_label = name_components[-1]
#      print("enforced_label")
#      print(enforced_label)
      data = pd.read_csv(fpath).to_numpy()

      bodydata[rpm] = []
      column_names[rpm] = []  
      for k in range(0,int(loop_number)):
        body_freq = data[:, 0]
        body_resp_1 = data[:, 4*k+1]
        body_resp_2 = data[:, 4*k+2]
        body_resp_3 = data[:, 4*k+3]
        body_resp_4 = data[:, 4*k+4] 
#        print("body_resp_1 k")  
#        print(fname)
#        print(k)    
#        print(data[2, 4*k+1])
        new_colnames = [column[4*k+1],column[4*k+2],column[4*k+3],column[4*k+4]]
        if len(bodydata[rpm]):
          bodydata[rpm].extend([body_resp_1, body_resp_2, body_resp_3, body_resp_4])
          column_names[rpm].extend(new_colnames)
        else:
          bodydata[rpm].append([body_freq,body_resp_1,body_resp_2,body_resp_3, body_resp_4])
          column_names[rpm].extend(["freq"] + new_colnames)
          
      for rpm, data in bodydata.items():
        dname = f"/BodyMount/N{enforced_nid}/{enforced_label.upper()}"
        print("dname")
        print(dname)
        collated_data = pd.DataFrame(np.vstack(data).T, columns=column_names[rpm])    
        print("collated_data")
        print(collated_data)
        dset = df.create_dataset(dname, data=cast_to_structured_array(collated_data))        
        
    response_ids_set = set(response_ids)
    df.attrs['response_id_body'] = list(response_ids_set & body_response_nids)  # ボディ関連


###########
    avldata = {}
    column_names = {}
    axis_labels = ("fx", "fy", "fz", "rx", "ry", "rz")
    avl_nids = node_ids[0, :].tolist()
    for fpath in avl_files:
      fname = os.path.splitext(os.path.basename(fpath))[0]
      name_components = fname.split("_")
      axis = axis_labels[int(name_components[-1][-1]) - 1]
      print("axis")
      print(axis)
      rpm = 0
      mount_nid = int(name_components[-2])
      print("mount_nid")
      print(mount_nid)      
      if mount_nid not in avl_nids:
        continue

      if rpm not in avldata:
        avldata[rpm] = []
        column_names[rpm] = []

      data = pd.read_csv(fpath, skiprows=[1]).to_numpy()
      avl_freq = data[:, 0]
      mag = data[:, 1]
      phase = np.radians(data[:, 2])
      # interp_data = np.interp(nast_freq, avl_freq, mag * np.cos(phase) + 1.j * mag * np.sin(phase) )
      avl_resp = mag * np.cos(phase) + 1.j * mag * np.sin(phase)
      new_colnames = [f"{mount_nid}.{axis}.{comp}" for comp in ("R", "I", "Mag", "Phase")]

      if len(avldata[rpm]):
        # avldata[rpm].extend([interp_data.real, interp_data.imag, np.abs(interp_data), np.angle(interp_data, True)])
        avldata[rpm].extend([avl_resp.real, avl_resp.imag, np.abs(avl_resp), np.angle(avl_resp, True)])
        column_names[rpm].extend(new_colnames)
      else:
        # avldata[rpm].append([nast_freq, interp_data.real, interp_data.imag, np.abs(interp_data), np.angle(interp_data, True)])
        avldata[rpm].append([avl_freq, avl_resp.real, avl_resp.imag, np.abs(avl_resp), np.angle(avl_resp, True)])
        column_names[rpm].extend(["freq"] + new_colnames)

    for rpm, data in avldata.items():
      dname = f"/AVLMount/{rpm}RPM"
      collated_data = pd.DataFrame(np.vstack(data).T, columns=column_names[rpm])
      dset = df.create_dataset(dname, data=cast_to_structured_array(collated_data))



  return




if __name__ == "__main__":
  pch_file = sys.argv[1]
  outputname = sys.argv[2]
  with PunchReader(pch_file) as f:
    data = f.parse()
    for subcase_id, subcase_data in data.items():
      outputfile = outputname + f"_{subcase_data['enforced_nid']}.{subcase_data['enforced_label']}.csv"
      subcase_data["response"].to_csv(outputfile)
